<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>datadr  R function reference</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <link href="assets/bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="assets/custom/custom.css" rel="stylesheet">
    <!-- font-awesome -->
    <link href="assets/font-awesome/css/font-awesome.min.css" rel="stylesheet">

    <!-- prism -->
    <link href="assets/prism/prism.css" rel="stylesheet">
    <link href="assets/prism/prism.r.css" rel="stylesheet">
    <script type='text/javascript' src='assets/prism/prism.js'></script>
    <script type='text/javascript' src='assets/prism/prism.r.js'></script>
    
    
    
    
    
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
    <![endif]-->
    
    <link href='http://fonts.googleapis.com/css?family=Lato' rel='stylesheet' type='text/css'>
    <!-- <link href='http://fonts.googleapis.com/css?family=Lustria' rel='stylesheet' type='text/css'> -->
    <link href='http://fonts.googleapis.com/css?family=Bitter' rel='stylesheet' type='text/css'>
    

    <!-- Fav and touch icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
      <link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
                    <link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
                                   <!-- <link rel="shortcut icon" href="ico/favicon.png"> -->
  </head>

  <body>

    <div class="container-narrow">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
           <li class=''><a href='index.html'>Docs</a></li><li class='active'><a href='functionref.html'>Function Ref</a></li><li><a href='https://github.com/hafen/datadr'>Github <i class='fa fa-github'></i></a></li>
        </ul>
        <p class="myHeader">datadr  R function reference</p>
      </div>

      <hr>

<div class="container-fluid">
   <div class="row-fluid">
   
   <div class="col-md-3 well">
   <ul class = "nav nav-list" id="toc">
   <li class='nav-header'>Contents</li> <li class="active">
   <a target="_self" class="nav-not-header" href="#packagemain">Package Info</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#adddata">addData</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#adult">adult</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#asdataframeddf">as.data.frame.ddf</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#aslistddo">as.list.ddo</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#bsv">bsv</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#charfilehash">charFileHash</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combcollect">combCollect</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combddo">combDdo</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combmean">combMean</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combmeancoef">combMeanCoef</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combrbind">combRbind</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#conddiv">condDiv</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#convert">convert</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#ddf-class">ddf-class</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#ddf">ddf</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#kvexample">kvExample</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#ddo">ddo</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#setattributes">setAttributes</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#digestfilehash">digestFileHash</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#divide">divide</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#dfsplit">dfSplit</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drblb">drBLB</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drfilter">drFilter</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drglm">drGLM</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drhexbin">drHexBin</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drjoin">drJoin</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drlapply">drLapply</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drreadtable">drRead.table</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drsample">drSample</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drxtabs">drXtabs</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#getbsv">getBsv</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#getbsvs">getBsvs</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#getsplitvar">getSplitVar</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#getsplitvars">getSplitVars</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#hdfsconn">hdfsConn</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#kvapply">kvApply</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#localdiskconn">localDiskConn</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#localdiskcontrol">localDiskControl</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#makeextractable">makeExtractable</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#mrexec">mrExec</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#tabulatemap">tabulateMap</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#printddo">print.ddo</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#quantileddf">quantile.ddf</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#readhdfstextfile">readHDFStextFile</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#readtextfilebychunk">readTextFileByChunk</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#recombine">recombine</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#removedata">removeData</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#rhipecontrol">rhipeControl</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#rrdiv">rrDiv</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#sparkcontrol">sparkControl</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#sparkdataconn">sparkDataConn</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#updateattributes">updateAttributes</a>
</li>
   </ul>
   </div>

<div class="col-md-9 tab-content" id="main-content">

<!-- packagemain -->   
<div class="tab-pane active" id="packagemain">

<h3>Divide and Recombine for Large, Complex Data</h3>

<p><strong>Author:</strong> Ryan Hafen</p>
<p><strong>Version:</strong> 0.6.9</p>
<p><strong>Date:</strong> 2014-03-15</p>
<p><strong>License:</strong> BSD_3_clause + file LICENSE</p>

<h4>Description</h4>
<p>Methods for dividing data into subsets, applying analytical
methods to the subsets, and recombining the results.  Comes with a generic
MapReduce interface as well.  Works with key-value pairs stored in memory,
on local disk, or on HDFS, in the latter case using the R and Hadoop
Integrated Programming Environment (RHIPE).  Also includes experimental
support for key-value pairs stored as Spark RDDs using the SparkR package.</p>

<h4>Depends</h4>
<p>
R (&gt;= 2.15.1),
methods,
parallel,
data.table,
digest,
codetools,
hexbin,
testthat</p>

<h4>Suggests</h4>
<p></p>

</div>
<!-- addData -->   
<div class="tab-pane" id="adddata">

<h3 class="fref_title">Add Key-Value Pairs to a Data Connection</h3>

<h4>Usage</h4>
<pre><code class="language-r">addData(conn, data, overwrite = FALSE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>conn</dt>
  <dd>a kvConnection object</dd>
  <dt>data</dt>
  <dd>a list of key-value pairs (list of lists
  where each sub-list has two elements, the key and the
  value)</dd>
  <dt>overwrite</dt>
  <dd>if data with the same key is already
  present in the data, should it be overwritten? (does not
  work for HDFS connections)</dd>
</dl>

  <h4>Description</h4>

  <p>Add key-value pairs to a data connection</p>


  <h4>Note</h4>

  <p>This is generally not recommended for HDFS as it writes a
new file each time it is called, and can result in more
individual files than Hadoop likes to deal with.</p>




<h4>See also</h4>

<code><a href='#removedata'>removeData</a></code>, <code><a href='#localdiskconn'>localDiskConn</a></code>,
<code><a href='#hdfsconn'>hdfsConn</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- adult -->   
<div class="tab-pane" id="adult">

<h3 class="fref_title">"Census Income" Dataset</h3>

<h4>Usage</h4>
<pre><code class="language-r">adult</code></pre>

  <h4>Format</h4>

  <p>(From UCI machine learning repository)</p>

  <p><ul>
<li> age. continuous
   </li>
<li> workclass. Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
   </li>
<li> fnlwgt. continuous
   </li>
<li> education. Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool
   education-num: continuous
   </li>
<li> marital. Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse
   </li>
<li> occupation. Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces
   </li>
<li> relationship. Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried
   </li>
<li> race. White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black
   </li>
<li> sex. Female, Male
   </li>
<li> capgain. continuous
   </li>
<li> caploss. continuous
   </li>
<li> hoursperweek. continuous
   </li>
<li> nativecountry. United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands
   </li>
<li> income. <=50K, >50K
   </li>
<li> incomebin. 0 if income<=50K, 1 if income>50K
</li>
</ul></p>


  <h4>&quot;Census Income&quot; Dataset</h4>


  <h4>Description</h4>

  <p>"Census Income" dataset from UCI machine learning
repository</p>


  <h4>References</h4>

  <p>Bache, K. & Lichman, M. (2013). UCI Machine Learning
Repository [<a href = 'http://archive.ics.uci.edu/ml'>http://archive.ics.uci.edu/ml</a>]. Irvine,
CA: University of California, School of Information and
Computer Science.</p>



</div>


<!-- as.data.frame.ddf -->   
<div class="tab-pane" id="asdataframeddf">

<h3 class="fref_title">Turn 'ddf' Object into Data Frame</h3>

<h4>Usage</h4>
<pre><code class="language-r">as.data.frame(x, row.names = NULL, optional = FALSE, keys = TRUE, splitVars = TRUE, 
  bsvs = FALSE, ...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddf' object</dd>
  <dt>row.names</dt>
  <dd>passed to <code>as.data.frame</code></dd>
  <dt>optional</dt>
  <dd>passed to <code>as.data.frame</code></dd>
  <dt>keys</dt>
  <dd>should the key be added as a variable in the
  resulting data frame? (if key is not a character, it will
  be replaced with a md5 hash)</dd>
  <dt>splitVars</dt>
  <dd>should the values of the splitVars be
  added as variables in the resulting data frame?</dd>
  <dt>bsvs</dt>
  <dd>should the values of bsvs be added as
  variables in the resulting data frame?</dd>
  <dt>...</dt>
  <dd>additional arguments passed to
  as.data.frame</dd>
</dl>

  <h4>Description</h4>

  <p>Rbind all the rows of a 'ddf' object into a single data
frame</p>



</div>


<!-- as.list.ddo -->   
<div class="tab-pane" id="aslistddo">

<h3 class="fref_title">Turn 'ddo' / 'ddf' Object into a list</h3>

<h4>Usage</h4>
<pre><code class="language-r">as.list(x, ...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddo' / 'ddf' object</dd>
  <dt>...</dt>
  <dd>additional arguments passed to
  <code>as.list</code></dd>
</dl>

  <h4>Description</h4>

  <p>Turn 'ddo' / 'ddf' Object into a list</p>



</div>


<!-- bsv -->   
<div class="tab-pane" id="bsv">

<h3 class="fref_title">Construct Between Subset Variable (BSV)</h3>

<h4>Usage</h4>
<pre><code class="language-r">bsv(val = NULL, desc = "")</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>val</dt>
  <dd>a scalar character, numeric, or date</dd>
  <dt>desc</dt>
  <dd>a character string describing the BSV</dd>
</dl>

  <h4>Description</h4>

  <p>Construct between subset variable (BSV)</p>


  <h4>Details</h4>

  <p>Should be called inside the <code>bsvFn</code> argument to
<code>divide</code> used for constructing a BSV list for each
subset of a division.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#getbsvs'>getBsvs</a></code>,
<code><a href='ddo-ddf-#accessors'>bsvInfo</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- charFileHash -->   
<div class="tab-pane" id="charfilehash">

<h3 class="fref_title">Character File Hash Function</h3>

<h4>Usage</h4>
<pre><code class="language-r">charFileHash(keys, conn)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>keys</dt>
  <dd>keys to be hashed</dd>
  <dt>conn</dt>
  <dd>a "localDiskConn" object</dd>
</dl>

  <h4>Description</h4>

  <p>Function to be used to specify the file where key-value
pairs get stored for local disk connections, useful when
keys are scalar strings.  Should be passed as the argument
<code>fileHashFn</code> to <code><a href='#localdiskconn'>localDiskConn</a></code>.</p>


  <h4>Details</h4>

  <p>You shouldn't need to call this directly other than to
experiment with what the output looks like or to get ideas
on how to write your own custom hash.</p>




<h4>See also</h4>

<code>localDiskConn</code>, <code><a href='#digestfilehash'>digestFileHash</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combCollect -->   
<div class="tab-pane" id="combcollect">

<h3 class="fref_title">"Collect" Recombination</h3>

<h4>Usage</h4>
<pre><code class="language-r">combCollect(...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Description</h4>

  <p>"Collect" recombination - simply collect the results into a
local list of key-value pairs</p>


  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as
the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>,
<code><a href='#combddo'>combDdo</a></code>, <code><a href='#combmeancoef'>combMeanCoef</a></code>,
<code><a href='#combrbind'>combRbind</a></code>, <code><a href='#combmean'>combMean</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combDdo -->   
<div class="tab-pane" id="combddo">

<h3 class="fref_title">"DDO" Recombination</h3>

<h4>Usage</h4>
<pre><code class="language-r">combDdo(...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Description</h4>

  <p>"DDO" recombination - simply collect the results into a
"ddo" object</p>


  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as
the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>,
<code><a href='#combcollect'>combCollect</a></code>, <code><a href='#combmeancoef'>combMeanCoef</a></code>,
<code><a href='#combrbind'>combRbind</a></code>, <code><a href='#combmean'>combMean</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combMean -->   
<div class="tab-pane" id="combmean">

<h3 class="fref_title">Mean Recombination</h3>

<h4>Usage</h4>
<pre><code class="language-r">combMean(...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Description</h4>

  <p>Mean recombination</p>


  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as
the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>,
<code><a href='#combcollect'>combCollect</a></code>, <code><a href='#combddo'>combDdo</a></code>,
<code><a href='#combrbind'>combRbind</a></code>, <code><a href='#combmeancoef'>combMeanCoef</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combMeanCoef -->   
<div class="tab-pane" id="combmeancoef">

<h3 class="fref_title">Mean Coefficient Recombination</h3>

<h4>Usage</h4>
<pre><code class="language-r">combMeanCoef(...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Description</h4>

  <p>Mean coefficient recombination</p>


  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as
the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.  It
expects to be dealing with named vectors including an
element <code>n</code> specifying the number of rows in that
subset.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>,
<code><a href='#rrdiv'>rrDiv</a></code>, <code><a href='#combcollect'>combCollect</a></code>,
<code><a href='#combddo'>combDdo</a></code>, <code><a href='#combrbind'>combRbind</a></code>,
<code><a href='#combmean'>combMean</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combRbind -->   
<div class="tab-pane" id="combrbind">

<h3 class="fref_title">"rbind" Recombination</h3>

<h4>Usage</h4>
<pre><code class="language-r">combRbind(...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Description</h4>

  <p>"rbind" recombination</p>


  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as
the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>,
<code><a href='#combddo'>combDdo</a></code>, <code><a href='#combcollect'>combCollect</a></code>,
<code><a href='#combmeancoef'>combMeanCoef</a></code>, <code><a href='#combmean'>combMean</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- condDiv -->   
<div class="tab-pane" id="conddiv">

<h3 class="fref_title">Conditioning Variable Division</h3>

<h4>Usage</h4>
<pre><code class="language-r">condDiv(vars)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>vars</dt>
  <dd>a character string or vector of character
  strings specifying the variables of the input data across
  which to divide</dd>
</dl>

  <h4>Value</h4>

  <p>a list to be used for the "by" argument to
<code><a href='#divide'>divide</a></code></p>


  <h4>Description</h4>

  <p>Specify conditioning variable division parameters for data
division</p>


  <h4>Details</h4>

  <p>Currently each unique combination of values of <code>vars</code>
constitutes a subset.  In the future, specifying shingles
for numeric conditioning variables will be implemented.</p>


  <h4>References</h4>

  <p><ul>
<li> <a href = 'http://www.datadr.org'>http://www.datadr.org</a> </li>
<li>
<a href = 'http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full'>Guha,
S., Hafen, R., Rounds, J., Xia, J., Li, J., Xi, B., &
Cleveland, W. S. (2012). Large complex data: divide and
recombine (D&R) with RHIPE. <em>Stat</em>, 1(1), 53-67.</a> </li>
</ul></p>

  <p></p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#getsplitvars'>getSplitVars</a></code>,
<code><a href='#getsplitvar'>getSplitVar</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- convert -->   
<div class="tab-pane" id="convert">

<h3 class="fref_title">Convert 'ddo' / 'ddf' Objects</h3>

<h4>Usage</h4>
<pre><code class="language-r">convert(from, to)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>from</dt>
  <dd>a 'ddo' or 'ddf' object</dd>
  <dt>to</dt>
  <dd>a 'kvConnection' object (created with
  <code><a href='#localdiskconn'>localDiskConn</a></code> or <code><a href='#hdfsconn'>hdfsConn</a></code>) or
  <code>NULL</code> if an in-memory 'ddo' / 'ddf' is desired</dd>
</dl>

  <h4>Description</h4>

  <p>Convert 'ddo' / 'ddf' objects between different storage
backends</p>



</div>


<!-- ddf-class -->   
<div class="tab-pane" id="ddf-class">

<h3 class="fref_title">'ddf' accessors</h3>

<h4>Usage</h4>
<pre><code class="language-r">nrow(x)

NROW(x)

ncol(x)

NCOL(x)

nrow(x)

NROW(x)

ncol(x)

NCOL(x)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddf' object</dd>
</dl>

  <h4>Description</h4>

  <p>'ddf' accessors</p>

  <p>'ddf' accessors</p>

  <p>'ddf' accessors</p>

  <p>'ddf' accessors</p>

  <p>The Number of Rows/Columns of a 'ddf' object</p>



</div>


<!-- ddf -->   
<div class="tab-pane" id="ddf">

<h3 class="fref_title">Instantiate a Distributed Data Frame ('ddf')
Instantiate a distributed data frame ('ddf')</h3>

<h4>Usage</h4>
<pre><code class="language-r">ddf(conn, transFn = identity, update = FALSE, reset = FALSE, control = NULL, verbose = TRUE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>conn</dt>
  <dd>an object pointing to where data is or will
  be stored for the 'ddf' object - can be a 'kvConnection'
  object created from <code><a href='#localdiskconn'>localDiskConn</a></code> or
  <code><a href='#hdfsconn'>hdfsConn</a></code>, or a data frame or list of
  key-value pairs</dd>
  <dt>transFn</dt>
  <dd>transFn a function to be applied to the
  key-value pairs of this data prior to doing any
  processing, that transform the data into a data frame if
  it is not stored as such</dd>
  <dt>update</dt>
  <dd>should the attributes of this object be
  updated?  See <code><a href='#updateattributes'>updateAttributes</a></code> for more
  details.</dd>
  <dt>reset</dt>
  <dd>should all persistent metadata about this
  object be removed and the object created from scratch?
  This setting does not effect data stored in the
  connection location.</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things if attributes are updated
  (most-likely parameters to <code>rhwatch</code> in RHIPE) - see
  <code><a href='#rhipecontrol'>rhipeControl</a></code> and
  <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is
  being done</dd>
</dl>

  <h4>Description</h4>

  <p>Instantiate a Distributed Data Frame ('ddf') Instantiate a
distributed data frame ('ddf')</p>



</div>


<!-- kvExample -->   
<div class="tab-pane" id="kvexample">

<h3 class="fref_title">Accessor Functions</h3>

<h4>Usage</h4>
<pre><code class="language-r">kvExample(x, transform = FALSE)

bsvInfo(x)

counters(x)

splitSizeDistn(x)

splitRowDistn(x)

getKeys(x)

summary(object, ...)

hasExtractableKV(x)

names(x)

length(x)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddf'/'ddo' object</dd>
  <dt>transform</dt>
  <dd>if the 'ddf' object has a
  <code>transFn</code>, should it be applied prior to returning?</dd>
  <dt>object</dt>
  <dd>a 'ddf'/'ddo' object</dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
</dl>

  <h4>Description</h4>

  <p>Accessor functions for attributes of ddo/ddf objects.
Methods also include <code>nrow</code> and <code>ncol</code> for ddf
objects.</p>

  <p>Accessor methods for 'ddo' and 'ddf' objects</p>



</div>


<!-- ddo -->   
<div class="tab-pane" id="ddo">

<h3 class="fref_title">Instantiate a Distributed Data Object ('ddo')
Instantiate a distributed data object ('ddo')</h3>

<h4>Usage</h4>
<pre><code class="language-r">ddo(conn, update = FALSE, reset = FALSE, control = NULL, verbose = TRUE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>conn</dt>
  <dd>an object pointing to where data is or will
  be stored for the 'ddf' object - can be a 'kvConnection'
  object created from <code><a href='#localdiskconn'>localDiskConn</a></code> or
  <code><a href='#hdfsconn'>hdfsConn</a></code>, or a data frame or list of
  key-value pairs</dd>
  <dt>update</dt>
  <dd>should the attributes of this object be
  updated?  See <code><a href='#updateattributes'>updateAttributes</a></code> for more
  details.</dd>
  <dt>reset</dt>
  <dd>should all persistent metadata about this
  object be removed and the object created from scratch?
  This setting does not effect data stored in the
  connection location.</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things if attributes are updated
  (most-likely parameters to <code>rhwatch</code> in RHIPE) - see
  <code><a href='#rhipecontrol'>rhipeControl</a></code> and
  <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is
  being done</dd>
</dl>

  <h4>Description</h4>

  <p>Instantiate a Distributed Data Object ('ddo') Instantiate a
distributed data object ('ddo')</p>



</div>


<!-- setAttributes -->   
<div class="tab-pane" id="setattributes">

<h3 class="fref_title">Managing attributes of 'ddo' or 'ddf' objects</h3>

<h4>Usage</h4>
<pre><code class="language-r">setAttributes(obj, ...)

setAttributes(obj, attrs)

setAttributes(obj, attrs)

getAttribute(obj, attrName)

getAttributes(obj, ...)

getAttributes(obj, attrNames)

getAttributes(obj, attrNames)

hasAttributes(obj, ...)

hasAttributes(obj, attrNames)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>attrs</dt>
  <dd>a named list of attributes to set</dd>
  <dt>obj</dt>
  <dd>'ddo' or 'ddf' object</dd>
  <dt>attrName</dt>
  <dd>name of the attribute to get</dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
  <dt>attrNames</dt>
  <dd>vector of names of the attributes to
  get</dd>
</dl>

  <h4>Description</h4>

  <p>Managing attributes of 'ddo' or 'ddf' objects</p>



</div>


<!-- digestFileHash -->   
<div class="tab-pane" id="digestfilehash">

<h3 class="fref_title">Digest File Hash Function</h3>

<h4>Usage</h4>
<pre><code class="language-r">digestFileHash(keys, conn)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>keys</dt>
  <dd>keys to be hashed</dd>
  <dt>conn</dt>
  <dd>a "localDiskConn" object</dd>
</dl>

  <h4>Description</h4>

  <p>Function to be used to specify the file where key-value
pairs get stored for local disk connections, useful when
keys are arbitrary objects.  File names are determined
using a md5 hash of the object.  This is the default
argument for <code>fileHashFn</code> in
<code><a href='#localdiskconn'>localDiskConn</a></code>.</p>


  <h4>Details</h4>

  <p>You shouldn't need to call this directly other than to
experiment with what the output looks like or to get ideas
on how to write your own custom hash.</p>




<h4>See also</h4>

<code>localDiskConn</code>, <code><a href='#charfilehash'>charFileHash</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- divide -->   
<div class="tab-pane" id="divide">

<h3 class="fref_title">Divide a Distributed Data Object</h3>

<h4>Usage</h4>
<pre><code class="language-r">divide(data, by = NULL, spill = 1e+06, filterFn = NULL, bsvFn = NULL, output = NULL, 
  overwrite = FALSE, preTransFn = NULL, postTransFn = NULL, params = NULL, control = NULL, 
  update = FALSE, verbose = TRUE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>an object of class "ddf" or "ddo" - in the
  latter case, need to specify <code>preTransFn</code> to coerce
  each subset into a data frame</dd>
  <dt>by</dt>
  <dd>specification of how to divide the data -
  conditional (factor-level or shingles), random replicate,
  or near-exact replicate (to come) -- see details</dd>
  <dt>bsvFn</dt>
  <dd>a function to be applied to each subset that
  returns a list of between subset variables (BSVs)</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where
  the output data should reside (see
  <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If
  <code>NULL</code> (default), output will be an in-memory "ddo"
  object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>spill</dt>
  <dd>integer telling the division method how many
  lines of data should be collected until spilling over
  into a new key-value pair</dd>
  <dt>filterFn</dt>
  <dd>a function that is applied to each
  candidate output key-value pair to determine whether it
  should be (if returns <code>TRUE</code>) part of the resulting
  division</dd>
  <dt>preTransFn</dt>
  <dd>a transformation function (if desired)
  to applied to each subset prior to division</dd>
  <dt>postTransFn</dt>
  <dd>a transformation function (if desired)
  to apply to each post-division subset</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in the distributed computing
  (most should be taken care of automatically such that
  this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>update</dt>
  <dd>should a MapReduce job be run to obtain
  additional attributes for the result data prior to
  returning?</dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is
  being done</dd>
</dl>

  <h4>Value</h4>

  <p>an object of class "ddf" if the resulting subsets are data
frames.  Otherwise, an object of class "ddo".</p>


  <h4>Description</h4>

  <p>Divide a ddo/ddf object into subsets based on different
criteria</p>


  <h4>Details</h4>

  <p>The division methods this function will support include
conditioning variable division for factors (implemented --
see <code><a href='#conddiv'>condDiv</a></code>), conditioning variable division
for numerical variables through shingles, random replicate
(implemented -- see <code><a href='#rrdiv'>rrDiv</a></code>), and near-exact
replicate.  If <code>by</code> is a vector of variable names, the
data will be divided by these variables.  Alternatively,
this can be specified by e.g.  <code>condDiv(c("var1",
"var2"))</code>.</p>


  <h4>References</h4>

  <p><ul>
<li> <a href = 'http://www.datadr.org'>http://www.datadr.org</a> </li>
<li>
<a href = 'http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full'>Guha,
S., Hafen, R., Rounds, J., Xia, J., Li, J., Xi, B., &
Cleveland, W. S. (2012). Large complex data: divide and
recombine (D&R) with RHIPE. <em>Stat</em>, 1(1), 53-67.</a> </li>
</ul></p>

  <p></p>




<h4>See also</h4>

<code><a href='#recombine'>recombine</a></code>, <code><a href='#ddo'>ddo</a></code>,
<code><a href='#ddf'>ddf</a></code>, <code><a href='#conddiv'>condDiv</a></code>,
<code><a href='#rrdiv'>rrDiv</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- dfSplit -->   
<div class="tab-pane" id="dfsplit">

<h3 class="fref_title">Functions used in divide()</h3>

<h4>Usage</h4>
<pre><code class="language-r">dfSplit(curDF, by, seed)

addSplitAttrs(curSplit, bsvFn, by, postTransFn = NULL)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>curDF,seed</dt>
  <dd>arguments</dd>
  <dt>curSplit,bsvFn,by,postTransFn</dt>
  <dd>arguments</dd>
</dl>

  <h4>Description</h4>

  <p>Functions used in divide()</p>



</div>


<!-- drBLB -->   
<div class="tab-pane" id="drblb">

<h3 class="fref_title">Bag of Little Bootstraps Recombination 'apply' Method</h3>

<h4>Usage</h4>
<pre><code class="language-r">drBLB(statistic, metric, R, n)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>statistic</dt>
  <dd>a function to apply to each subset
  specifying the statistic to compute.  Must have arguments
  'data' and 'weights' - see details).  Must return a
  vector, where each element is a statistic of interest.</dd>
  <dt>metric</dt>
  <dd>a function specifying the metric to be
  applied to the <code>R</code> bootstrap samples of each
  statistic returned by <code>statistic</code>.  Expects an input
  vector and should output a vector.</dd>
  <dt>R</dt>
  <dd>the number of bootstrap samples</dd>
  <dt>n</dt>
  <dd>the total number of observations in the data</dd>
</dl>

  <h4>Description</h4>

  <p>Bag of little bootstraps recombination 'apply' method</p>


  <h4>Details</h4>

  <p>It is necessary to specify <code>weights</code> as a parameter to
the <code>statistic</code> function because for BLB to work
efficiently, it must resample each time with a sample of
size <code>n</code>.  To make this computationally possible for
very large <code>n</code>, we can use <code>weights</code> (see
reference for details).  Therefore, only methods with a
weights option can legitimately be used here.</p>


  <h4>References</h4>

  <p>BLB paper</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drFilter -->   
<div class="tab-pane" id="drfilter">

<h3 class="fref_title">Filter a 'ddo' or 'ddf' Object</h3>

<h4>Usage</h4>
<pre><code class="language-r">drFilter(x, filterFn, output = NULL, overwrite = FALSE, params = NULL, control = NULL)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>an object of class 'ddo' or 'ddf'</dd>
  <dt>filterFn</dt>
  <dd>function that takes the keys and/or
  values and returns either <code>TRUE</code> or <code>FALSE</code> -
  if <code>TRUE</code>, that key-value pair will be present in
  the result</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where
  the output data should reside (see
  <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If
  <code>NULL</code> (default), output will be an in-memory "ddo"
  object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in the distributed computing
  (most should be taken care of automatically such that
  this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>a 'ddo' or 'ddf' object</p>


  <h4>Description</h4>

  <p>Filter a 'ddo' or 'ddf' object</p>




<h4>See also</h4>

<code><a href='#drjoin'>drJoin</a></code>, <code><a href='#drlapply'>drLapply</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drGLM -->   
<div class="tab-pane" id="drglm">

<h3 class="fref_title">GLM Recombination 'apply' Method</h3>

<h4>Usage</h4>
<pre><code class="language-r">drGLM(...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>arguments you would pass to the
  <code><a href='http://www.inside-r.org/r-doc/stats/glm'>glm</a></code> function</dd>
</dl>

  <h4>Description</h4>

  <p>GLM recombination 'apply' method</p>


  <h4>Details</h4>

  <p>This provides a function to be called for each subset in a
recombination MapReduce job that applies R's glm method and
outputs the coefficients.  It is to be passed as the
argument <code>method</code> to <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>,
<code><a href='#rrdiv'>rrDiv</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drHexBin -->   
<div class="tab-pane" id="drhexbin">

<h3 class="fref_title">HexBin Aggregation for Distributed Data Frames</h3>

<h4>Usage</h4>
<pre><code class="language-r">drHexBin(data, xVar, yVar, xTransFn = identity, yTransFn = identity, xBins = 30, 
  shape = 1, params = NULL, control = NULL)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>a</dt>
  <dd>distributed data frame</dd>
  <dt>xVar,yVar</dt>
  <dd>names of the variables to use</dd>
  <dt>xTransFn,yTransFn</dt>
  <dd>a transformation function to
  apply to the x and y variables prior to binning</dd>
  <dt>xBins</dt>
  <dd>the number of bins partitioning the range of
  xbnds</dd>
  <dt>shape</dt>
  <dd>the shape = yheight/xwidth of the plotting
  regions</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in the distributed computing
  (most should be taken care of automatically such that
  this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>a "hexbin" object</p>


  <h4>Description</h4>

  <p>Create "hexbin" object of hexagonally binned data for a
distributed data frame.  This computation is division
agnostic - it does not matter how the data frame is split
up.</p>


  <h4>References</h4>

  <p>Carr, D. B. et al. (1987) Scatterplot Matrix Techniques for
Large <code class = 'eq'>N</code>. <em>JASA</em> <b>83</b>, 398, 424--436.</p>




<h4>See also</h4>

<code><a href='#quantileddf'>quantile.ddf</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drJoin -->   
<div class="tab-pane" id="drjoin">

<h3 class="fref_title">Join Two Data Sources by Key</h3>

<h4>Usage</h4>
<pre><code class="language-r">drJoin(..., output = NULL, overwrite = FALSE, postTransFn = NULL, params = NULL, 
  control = NULL)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>named lists of input objects - assumed that
  all are of same type (all HDFS, all localDisk, all
  in-memory)</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where
  the output data should reside (see
  <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If
  <code>NULL</code> (default), output will be an in-memory "ddo"
  object.</dd>
  <dt>postTransFn</dt>
  <dd>an optional function to be applied to
  the each final key-value pair after joining</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in the distributed computing
  (most should be taken care of automatically such that
  this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>a 'ddo' object stored in the <code>output</code> connection,
where the values are named lists with names according to
the names given to the input data objects, and values are
the corresponding data</p>


  <h4>Description</h4>

  <p>Join two data sources by key</p>




<h4>See also</h4>

<code><a href='#drfilter'>drFilter</a></code>, <code><a href='#drlapply'>drLapply</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drLapply -->   
<div class="tab-pane" id="drlapply">

<h3 class="fref_title">Apply a function to all key/value pairs of a ddo/ddf object</h3>

<h4>Usage</h4>
<pre><code class="language-r">drLapply(data, apply = NULL, combine = combDdo(), output = NULL, overwrite = FALSE, 
  params = NULL, control = NULL, verbose = TRUE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>an object of class "ddo" of "ddf"</dd>
  <dt>apply</dt>
  <dd>a function to be applied to each subset</dd>
  <dt>combine</dt>
  <dd>optional method to combine the results</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where
  the output data should reside (see
  <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If
  <code>NULL</code> (default), output will be an in-memory "ddo"
  object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in the distributed computing
  (most should be taken care of automatically such that
  this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is
  being done</dd>
</dl>

  <h4>Value</h4>

  <p>depends on <code>combine</code></p>


  <h4>Description</h4>

  <p>Apply a function to all key/value pairs of a ddo/ddf object
and get a new ddo object back, unless a different
<code>combine</code> strategy is specified.</p>




<h4>See also</h4>

<code><a href='#recombine'>recombine</a></code>, <code><a href='#drfilter'>drFilter</a></code>,
<code><a href='#drjoin'>drJoin</a></code>, <code><a href='#combddo'>combDdo</a></code>,
<code><a href='#combrbind'>combRbind</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drRead.table -->   
<div class="tab-pane" id="drreadtable">

<h3 class="fref_title">Data Input</h3>

<h4>Usage</h4>
<pre><code class="language-r">drRead.table(file, header = FALSE, sep = "", quote = "\"'", dec = ".", skip = 0, 
  fill = !blank.lines.skip, blank.lines.skip = TRUE, comment.char = "#", allowEscapes = FALSE, 
  encoding = "unknown", autoColClasses = TRUE, rowsPerBlock = 50000, postTransFn = identity, 
  output = NULL, overwrite = FALSE, params = NULL, control = NULL, ...)

drRead.csv(file, header = TRUE, sep = ",", quote = "\"", dec = ".", fill = TRUE, 
  comment.char = "", ...)

drRead.csv2(file, header = TRUE, sep = ";", quote = "\"", dec = ",", fill = TRUE, 
  comment.char = "", ...)

drRead.delim(file, header = TRUE, sep = "\t", quote = "\"", dec = ".", fill = TRUE, 
  comment.char = "", ...)

drRead.delim2(file, header = TRUE, sep = "\t", quote = "\"", dec = ",", fill = TRUE, 
  comment.char = "", ...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>file</dt>
  <dd>input text file - can either be character
  string pointing to a file on local disk, or an
  <code><a href='#hdfsconn'>hdfsConn</a></code> object pointing to a text file on
  HDFS (see <code>output</code> argument below)</dd>
  <dt>header</dt>
  <dd>this and parameters other parameters below
  are passed to <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for each chunk
  being processed - see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more
  info.  Most all have defaults or appropriate defaults are
  set through other format-specific functions such as
  <code>drRead.csv</code> and <code>drRead.delim</code>.</dd>
  <dt>sep</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more info</dd>
  <dt>quote</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more info</dd>
  <dt>dec</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more info</dd>
  <dt>skip</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more info</dd>
  <dt>fill</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more info</dd>
  <dt>blank.lines.skip</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for
  more info</dd>
  <dt>comment.char</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more
  info</dd>
  <dt>allowEscapes</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more
  info</dd>
  <dt>encoding</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more
  info</dd>
  <dt>...</dt>
  <dd>see <code><a href='http://www.inside-r.org/r-doc/utils/read.table'>read.table</a></code> for more info</dd>
  <dt>autoColClasses</dt>
  <dd>should column classes be determined
  automatically by reading in a sample?  This can sometimes
  be problematic because of strange ways R handles quotes
  in <code>read.table</code>, but keeping the default of
  <code>TRUE</code> is advantageous for speed.</dd>
  <dt>rowsPerBlock</dt>
  <dd>how many rows of the input file
  should make up a block (key-value pair) of output?</dd>
  <dt>postTransFn</dt>
  <dd>a function to be applied after a block
  is read in to provide any additional processingn before
  the block is stored</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where
  the output data should reside.  Must be a
  <code><a href='#localdiskconn'>localDiskConn</a></code> object if input is a text
  file on local disk, or a <code><a href='#hdfsconn'>hdfsConn</a></code> object if
  input is a text file on HDFS.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in <code>postTransFn</code></dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>an object of class "ddf"</p>


  <h4>Description</h4>

  <p>Reads a text file in table format and creates a distributed
data frame from it, with cases corresponding to lines and
variables to fields in the file.</p>


  <h4>Note</h4>

  <p>For local disk, the file is actually read in sequentially
instead of in parallel.  This is because of possible
performance issues when trying to read from the same disk
in parallel.</p>

  <p>Note that if <code>skip</code> is positive and/or if
<code>header</code> is <code>TRUE</code>, it will first read these in
as they only occur once in the data, and we then check for
these lines in each block and remove those lines if they
appear.</p>

  <p>Also note that if you supply <code>"Factor"</code> column
classes, they will be converted to character.</p>




<h4>Author</h4>

Ryan Hafen

</div>


<!-- drSample -->   
<div class="tab-pane" id="drsample">

<h3 class="fref_title">Take a Sample of Key-Value Pairs
Take a sample of key-value Pairs</h3>

<h4>Usage</h4>
<pre><code class="language-r">drSample(x, fraction, output = NULL, overwrite = FALSE, control = NULL)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddo' or 'ddf' object</dd>
  <dt>fraction</dt>
  <dd>fraction of key-value pairs to keep
  (between 0 and 1)</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where
  the output data should reside (see
  <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If
  <code>NULL</code> (default), output will be an in-memory "ddo"
  object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Description</h4>

  <p>Take a Sample of Key-Value Pairs Take a sample of key-value
Pairs</p>



</div>


<!-- drXtabs -->   
<div class="tab-pane" id="drxtabs">

<h3 class="fref_title">Division-Agnostic Tabulation</h3>

<h4>Usage</h4>
<pre><code class="language-r">drXtabs(formula, data = data, by = NULL, transFn = NULL, maxUnique = NULL, params = NULL, 
  control = NULL)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>formula</dt>
  <dd>a <code><a href='http://www.inside-r.org/r-doc/stats/formula'>formula</a></code> object with the
  cross-classifying variables (separated by +) on the right
  hand side (or an object which can be coerced to a
  formula). Interactions are not allowed. On the left hand
  side, one may optionally give a variable name in the data
  representing counts; in the latter case, the columns are
  interpreted as corresponding to the levels of a variable.
  This is useful if the data have already been tabulated.</dd>
  <dt>data</dt>
  <dd>a "ddf" containing the variables in the
  formula <code>formula</code></dd>
  <dt>by</dt>
  <dd>an optional variable by which to split up
  tabulations (i.e. tabulate independently inside of each
  unique "by" variable value).  The only difference between
  specifying "by" and placing it in the right hand side of
  the formula is how the computation is done and how the
  result is returned.</dd>
  <dt>transFn</dt>
  <dd>an optional function to apply to each
  subset prior to performing tabulation.  The output from
  this function should be a data frame containing variables
  with names that match that of the formula provided.</dd>
  <dt>maxUnique</dt>
  <dd>the maximum number of unique
  combinations of variables to obtaion tabulations for.
  This is meant to help against cases where a variable in
  the formula has a very large number of levels, to the
  point that it is not meaningful to tabulate and is too
  computationally burdonsome.  If <code>NULL</code>, it is
  ignored.  If a positive number, only the top and bottom
  <code>maxUnique</code> tabulations by frequency are kept.</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in the distributed computing
  (most should be taken care of automatically such that
  this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>a data frame of the tabulations.  When "by" is specified,
it is a named list, with each element corresponding to a
unique "by" value, containing a data frame of tabulations.</p>


  <h4>Description</h4>

  <p>Creates contingency tables from cross-classifying factors,
with a formula interface similar to <code>xtabs</code></p>




<h4>See also</h4>

<code><a href='http://www.inside-r.org/r-doc/stats/xtabs'>xtabs</a></code>, <code><a href='#updateattributes'>updateAttributes</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- getBsv -->   
<div class="tab-pane" id="getbsv">

<h3 class="fref_title">Get Between Subset Variable</h3>

<h4>Usage</h4>
<pre><code class="language-r">getBsv(x, name)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a key-value pair or a value</dd>
  <dt>name</dt>
  <dd>the name of the BSV to get</dd>
</dl>

  <h4>Description</h4>

  <p>For a given key-value pair, get a BSV variable value by
name (if present)</p>



</div>


<!-- getBsvs -->   
<div class="tab-pane" id="getbsvs">

<h3 class="fref_title">Get Between Subset Variables</h3>

<h4>Usage</h4>
<pre><code class="language-r">getBsvs(x)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a key-value pair or a value</dd>
</dl>

  <h4>Description</h4>

  <p>For a given key-value pair, exract all BSVs</p>



</div>


<!-- getSplitVar -->   
<div class="tab-pane" id="getsplitvar">

<h3 class="fref_title">Extract "Split" Variable</h3>

<h4>Usage</h4>
<pre><code class="language-r">getSplitVar(x, name)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a key-value pair or a value</dd>
  <dt>name</dt>
  <dd>the name of the split variable to get</dd>
</dl>

  <h4>Description</h4>

  <p>For a given key-value pair or value, get a split variable
value by name, if present (split variables are variables
that define how the data was divided).</p>



</div>


<!-- getSplitVars -->   
<div class="tab-pane" id="getsplitvars">

<h3 class="fref_title">Extract "Split" Variables</h3>

<h4>Usage</h4>
<pre><code class="language-r">getSplitVars(x)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a key-value pair or a value</dd>
</dl>

  <h4>Description</h4>

  <p>For a given k/v pair or value, exract all split variables
(split variables are variables that define how the data was
divided).</p>



</div>


<!-- hdfsConn -->   
<div class="tab-pane" id="hdfsconn">

<h3 class="fref_title">Connect to Data Source on HDFS</h3>

<h4>Usage</h4>
<pre><code class="language-r">hdfsConn(loc, type = "sequence", autoYes = FALSE, reset = FALSE, verbose = TRUE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>loc</dt>
  <dd>location on HDFS for the data source</dd>
  <dt>type</dt>
  <dd>the type of data ("map", "sequence", "text")</dd>
  <dt>autoYes</dt>
  <dd>automatically answer "yes" to questions
  about creating a path on HDFS</dd>
  <dt>reset</dt>
  <dd>should existing metadata for this object be
  overwritten?</dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is
  being done</dd>
</dl>

  <h4>Value</h4>

  <p>a "kvConnection" object of class "hdfsConn"</p>


  <h4>Description</h4>

  <p>Connect to a data source on HDFS</p>


  <h4>Details</h4>

  <p>This simply creates a "connection" to a directory on HDFS
(which need not have data in it).  To actually do things
with this data, see <code><a href='#ddo'>ddo</a></code>, etc.</p>




<h4>See also</h4>

<code>addData</code>, <code><a href='#ddo'>ddo</a></code>, <code><a href='#ddf'>ddf</a></code>,
<code><a href='#localdiskconn'>localDiskConn</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- kvApply -->   
<div class="tab-pane" id="kvapply">

<h3 class="fref_title">Apply Function to Key-Value Pair</h3>

<h4>Usage</h4>
<pre><code class="language-r">kvApply(fn, kvPair, returnKV = FALSE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>fn</dt>
  <dd>a function</dd>
  <dt>kvPair</dt>
  <dd>a key-value pair (a list with 2 elements)</dd>
  <dt>returnKV</dt>
  <dd>should the key and value be returned?</dd>
</dl>

  <h4>Description</h4>

  <p>Apply a function to a single key-value pair - not a
traditional R "apply" function.</p>


  <h4>Details</h4>

  <p>Determines how a function should be applied to a key-value
pair and then applies it: if the function has two formals,
it applies the function giving it the key and the value as
the arguments; if the function has one formal, it applies
the function giving it just the value.  This provides
flexibility and simplicity for when a function is only
meant to be applied to the value, but still allows keys to
be used if desired.</p>



</div>


<!-- localDiskConn -->   
<div class="tab-pane" id="localdiskconn">

<h3 class="fref_title">Connect to Data Source on Local Disk</h3>

<h4>Usage</h4>
<pre><code class="language-r">localDiskConn(loc, nBins = 0, fileHashFn = NULL, autoYes = FALSE, reset = FALSE, 
  verbose = TRUE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>loc</dt>
  <dd>location on local disk for the data source</dd>
  <dt>nBins</dt>
  <dd>number of bins (subdirectories) to put data
  files into - if anticipating a large number of k/v pairs,
  it is a good idea to set this to something bigger than 0</dd>
  <dt>fileHashFn</dt>
  <dd>an optional function that operates on
  each key-value pair to determine the subdirectory
  structure for where the data should be stored for that
  subset, or can be specified "asis" when keys are scalar
  strings</dd>
  <dt>autoYes</dt>
  <dd>automatically answer "yes" to questions
  about creating a path on local disk</dd>
  <dt>reset</dt>
  <dd>should existing metadata for this object be
  overwritten?</dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is
  being done</dd>
</dl>

  <h4>Value</h4>

  <p>a "kvConnection" object of class "localDiskConn"</p>


  <h4>Description</h4>

  <p>Connect to a data source on local disk</p>


  <h4>Details</h4>

  <p>This simply creates a "connection" to a directory on local
disk (which need not have data in it).  To actually do
things with this connection, see <code><a href='#ddo'>ddo</a></code>, etc.
Typically, you should just use <code>loc</code> to specify where
the data is or where you would like data for this
connection to be stored.  Metadata for the object is also
stored in this directory.</p>




<h4>See also</h4>

<code>addData</code>, <code><a href='#ddo'>ddo</a></code>, <code><a href='#ddf'>ddf</a></code>,
<code><a href='#localdiskconn'>localDiskConn</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- localDiskControl -->   
<div class="tab-pane" id="localdiskcontrol">

<h3 class="fref_title">Specify Control Parameters for MapReduce on a Local Disk Connection</h3>

<h4>Usage</h4>
<pre><code class="language-r">localDiskControl(cluster = NULL, map_buff_size_bytes = 10485760, reduce_buff_size_bytes = 10485760, 
  map_temp_buff_size_bytes = 10485760)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>cluster</dt>
  <dd>a "cluster" object obtained from
  <code><a href='http://www.inside-r.org/r-doc/parallel/makeCluster'>makeCluster</a></code> to allow for parallel
  processing</dd>
  <dt>map_buff_size_bytes</dt>
  <dd>determines how much data
  should be sent to each map task</dd>
  <dt>reduce_buff_size_bytes</dt>
  <dd>determines how much data
  should be sent to each reduce task</dd>
  <dt>map_temp_buff_size_bytes</dt>
  <dd>determines the size of
  chunks written to disk in between the map and reduce</dd>
</dl>

  <h4>Description</h4>

  <p>Specify control parameters for a MapReduce on a local disk
connection.  Currently the parameters include:</p>


  <h4>Note</h4>

  <p>If you have data on a shared drive that multiple nodes can
access or a high performance shared file system like
Lustre, you can run a local disk MapReduce job on multiple
nodes by creating a multi-node cluster with
<code><a href='http://www.inside-r.org/r-doc/parallel/makeCluster'>makeCluster</a></code>.</p>

  <p>If you are using multiple cores and the input data is very
small, <code>map_buff_size_bytes</code> needs to be small so that
the key-value pairs will be split across cores.</p>



</div>


<!-- makeExtractable -->   
<div class="tab-pane" id="makeextractable">

<h3 class="fref_title">Take a ddo/ddf HDFS data object and turn it into a mapfile</h3>

<h4>Usage</h4>
<pre><code class="language-r">makeExtractable(obj)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>obj</dt>
  <dd>object of class 'ddo' or 'ddf' with an HDFS
  connection</dd>
</dl>

  <h4>Description</h4>

  <p>Take a ddo/ddf HDFS data object and turn it into a mapfile</p>



</div>


<!-- mrExec -->   
<div class="tab-pane" id="mrexec">

<h3 class="fref_title">Execute a MapReduce Job</h3>

<h4>Usage</h4>
<pre><code class="language-r">mrExec(data, setup = NULL, map = NULL, reduce = NULL, output = NULL, overwrite = FALSE, 
  control = NULL, params = NULL, verbose = TRUE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>a ddo/ddf object, or list of ddo/ddf objects</dd>
  <dt>setup</dt>
  <dd>an expression of R code (created using the R
  command <code>expression</code>) to be run before map and
  reduce</dd>
  <dt>map</dt>
  <dd>an R expression that is evaluated during the
  map stage. For each task, this expression is executed
  multiple times (see details).</dd>
  <dt>reduce</dt>
  <dd>a vector of R expressions with names pre,
  reduce, and post that is evaluated during the reduce
  stage. For example <code>reduce = expression(pre = {...},
  reduce = {...}, post = {...})</code>. reduce is optional, and
  if not specified the map output key-value pairs will be
  the result. If it is not specified, then a default
  identity reduce is performed. Setting it to 0 will skip
  the reduce altogether.</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where
  the output data should reside (see
  <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If
  <code>NULL</code> (default), output will be an in-memory "ddo"
  object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in the map or reduce phases</dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is
  being done</dd>
</dl>

  <h4>Value</h4>

  <p>"ddo" object - to keep it simple.  It is up to the user to
update or cast as "ddf" if that is the desired result.</p>


  <h4>Description</h4>

  <p>Execute a MapReduce job</p>




<h4>Author</h4>

Ryan Hafen

</div>


<!-- tabulateMap -->   
<div class="tab-pane" id="tabulatemap">

<h3 class="fref_title">Functions to Compute Summary Statistics in MapReduce</h3>

<h4>Usage</h4>
<pre><code class="language-r">tabulateMap(formula, data)

tabulateReduce(result, reduce.values, maxUnique = NULL)

calculateMoments(y, order = 1, na.rm = TRUE)

combineMoments(m1, m2)

combineMultipleMoments(...)

moments2statistics(m)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>formula</dt>
  <dd>a formula to be used in
  <code><a href='http://www.inside-r.org/r-doc/stats/xtabs'>xtabs</a></code></dd>
  <dt>data</dt>
  <dd>a subset of a 'ddf' object</dd>
  <dt>maxUnique</dt>
  <dd>the maximum number of unique
  combinations of variables to obtaion tabulations for.
  This is meant to help against cases where a variable in
  the formula has a very large number of levels, to the
  point that it is not meaningful to tabulate and is too
  computationally burdonsome.  If <code>NULL</code>, it is
  ignored.  If a positive number, only the top and bottom
  <code>maxUnique</code> tabulations by frequency are kept.</dd>
  <dt>result,reduce.values</dt>
  <dd>inconsequential
  <code>tabulateReduce</code> parameters</dd>
  <dt>y,order,na.rm</dt>
  <dd>inconsequential
  <code>calculateMoments</code> parameters</dd>
  <dt>m1,m2</dt>
  <dd>inconsequential <code>combineMoments</code>
  parameters</dd>
  <dt>...</dt>
  <dd>inconsequential parameters</dd>
  <dt>m</dt>
  <dd>inconsequential <code>moments2statistics</code>
  parameters</dd>
</dl>

  <h4>Description</h4>

  <p>Functions that are used to tabulate categorical variables
and compute moments for numeric variables inside through
the MapReduce framework.  Used in
<code><a href='#updateattributes'>updateAttributes</a></code>.</p>



</div>


<!-- print.ddo -->   
<div class="tab-pane" id="printddo">

<h3 class="fref_title">Print a "ddo" or "ddf" Object</h3>

<h4>Usage</h4>
<pre><code class="language-r">print(x, ...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>object to be printed</dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
</dl>

  <h4>Description</h4>

  <p>Print an overview of attributes of distributed data objects
(ddo) or distributed data frames (ddf)</p>




<h4>Author</h4>

Ryan Hafen

</div>


<!-- quantile.ddf -->   
<div class="tab-pane" id="quantileddf">

<h3 class="fref_title">Sample Quantiles for 'ddf' Objects</h3>

<h4>Usage</h4>
<pre><code class="language-r">quantile(x, var, by = NULL, probs = seq(0, 1, 0.005), preTransFn = identity, varTransFn = identity, 
  nBins = 10000, tails = 100, params = NULL, control = NULL, ...)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddf' object</dd>
  <dt>var</dt>
  <dd>the name of the variable to compute quantiles
  for</dd>
  <dt>by</dt>
  <dd>the (optional) variable by which to group
  quantile computations</dd>
  <dt>probs</dt>
  <dd>numeric vector of probabilities with values
  in [0-1]</dd>
  <dt>preTransFn</dt>
  <dd>a transformation function (if desired)
  to applied to each subset prior to division (here it may
  be useful for adding a "by" variable that is not present)
  - note: this transformation should not modify <code>var</code>
  (use <code>varTransFn</code> for that)</dd>
  <dt>varTransFn</dt>
  <dd>transformation to apply to variable
  prior to computing quantiles</dd>
  <dt>nBins</dt>
  <dd>how many bins should the range of the
  variable be split into?</dd>
  <dt>tails</dt>
  <dd>how many exact values at each tail should be
  retained?</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in the distributed computing
  (most should be taken care of automatically such that
  this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
</dl>

  <h4>Value</h4>

  <p>data frame of quantiles <code>q</code> and their associated
f-value <code>fval</code>.  If <code>by</code> is specified, then also
a variable <code>group</code>.</p>


  <h4>Description</h4>

  <p>Compute sample quantiles for 'ddf' objects</p>


  <h4>Details</h4>

  <p>This division-agnostic quantile calculation algorithm takes
the range of the variable of interest and splits it into
<code>nBins</code> bins, tabulates counts for those bins, and
reconstructs a quantile approximation from them.
<code>nBins</code> should not get too large, but larger
<code>nBins</code> gives more accuracy.  If <code>tails</code> is
positive, the first and last <code>tails</code> ordered values
are attached to the quantile estimate - this is useful for
long-tailed distributions or distributions with outliers
for which you would like more detail in the tails.</p>




<h4>See also</h4>

<code><a href='#updateattributes'>updateAttributes</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- readHDFStextFile -->   
<div class="tab-pane" id="readhdfstextfile">

<h3 class="fref_title">Experimental HDFS text reader helper function</h3>

<h4>Usage</h4>
<pre><code class="language-r">readHDFStextFile(input, output = NULL, overwrite = FALSE, fn = NULL, keyFn = NULL, 
  linesPerBlock = 10000, control = NULL, update = FALSE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>input</dt>
  <dd>a RHIPE input text handle created with
  <code>rhfmt</code></dd>
  <dt>output</dt>
  <dd>an output connection such as those created
  with <code><a href='#localdiskconn'>localDiskConn</a></code>, and
  <code><a href='#hdfsconn'>hdfsConn</a></code></dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>fn</dt>
  <dd>function to be applied to each chunk of lines
  (input to function is a vector of strings)</dd>
  <dt>keyFn</dt>
  <dd>optional function to determine the value of
  the key for each block</dd>
  <dt>linesPerBlock</dt>
  <dd>how many lines at a time to read</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>update</dt>
  <dd>should a MapReduce job be run to obtain
  additional attributes for the result data prior to
  returning?</dd>
</dl>

  <h4>Description</h4>

  <p>Experimental helper function for reading text data on HDFS
into a HDFS connection</p>



</div>


<!-- readTextFileByChunk -->   
<div class="tab-pane" id="readtextfilebychunk">

<h3 class="fref_title">Experimental sequential text reader helper function</h3>

<h4>Usage</h4>
<pre><code class="language-r">readTextFileByChunk(input, output, overwrite = FALSE, linesPerBlock = 10000, fn = NULL, 
  header = TRUE, skip = 0, recordEndRegex = NULL, cl = NULL)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>input</dt>
  <dd>the path to an input text file</dd>
  <dt>output</dt>
  <dd>an output connection such as those created
  with <code><a href='#localdiskconn'>localDiskConn</a></code>, and
  <code><a href='#hdfsconn'>hdfsConn</a></code></dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>linesPerBlock</dt>
  <dd>how many lines at a time to read</dd>
  <dt>fn</dt>
  <dd>function to be applied to each chunk of lines
  (see details)</dd>
  <dt>header</dt>
  <dd>does the file have a header</dd>
  <dt>skip</dt>
  <dd>number of lines to skip before reading</dd>
  <dt>recordEndRegex</dt>
  <dd>an optional regular expression that
  finds lines in the text file that indicate the end of a
  record (for multi-line records)</dd>
  <dt>cl</dt>
  <dd>a "cluster" object to be used for parallel
  processing, created using <code>makeCluster</code></dd>
</dl>

  <h4>Description</h4>

  <p>Experimental helper function for reading text data
sequentially from a file on disk and adding to connection
using <code><a href='#adddata'>addData</a></code></p>


  <h4>Details</h4>

  <p>The function <code>fn</code> should have one argument, which
should expect to receive a vector of strings, each element
of which is a line in the file.  It is also possible for
<code>fn</code> to take two arguments, in which case the second
argument is the header line from the file (some parsing
methods might need to know the header).</p>



</div>


<!-- recombine -->   
<div class="tab-pane" id="recombine">

<h3 class="fref_title">Recombine</h3>

<h4>Usage</h4>
<pre><code class="language-r">recombine(data, apply = NULL, combine = NULL, output = NULL, overwrite = FALSE, params = NULL, 
  control = NULL, verbose = TRUE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>an object of class "ddo" of "ddf"</dd>
  <dt>apply</dt>
  <dd>a function specifying the analytic method to
  apply to each subset, or a pre-defined apply function
  (see <code><a href='#drblb'>drBLB</a></code>, <code><a href='#drglm'>drGLM</a></code>, for
  example)</dd>
  <dt>combine</dt>
  <dd>the method to combine the results</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where
  the output data should reside (see
  <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If
  <code>NULL</code> (default), output will be an in-memory "ddo"
  object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location
  be overwritten? (also can specify <code>overwrite =
  "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the
  input data that are needed in the distributed computing
  (most should be taken care of automatically such that
  this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code>
  and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is
  being done</dd>
</dl>

  <h4>Value</h4>

  <p>depends on <code>combine</code></p>


  <h4>Description</h4>

  <p>Apply an analytic recombination method to a ddo/ddf object
and combine the results</p>


  <h4>References</h4>

  <p><ul>
<li> <a href = 'http://www.datadr.org'>http://www.datadr.org</a> </li>
<li>
<a href = 'http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full'>Guha,
S., Hafen, R., Rounds, J., Xia, J., Li, J., Xi, B., &
Cleveland, W. S. (2012). Large complex data: divide and
recombine (D&R) with RHIPE. <em>Stat</em>, 1(1), 53-67.</a> </li>
</ul></p>

  <p></p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#ddo'>ddo</a></code>, <code><a href='#ddf'>ddf</a></code>,
<code><a href='#drglm'>drGLM</a></code>, <code><a href='#drblb'>drBLB</a></code>,
<code><a href='#combmeancoef'>combMeanCoef</a></code>, <code><a href='#combmean'>combMean</a></code>,
<code><a href='#combcollect'>combCollect</a></code>, <code><a href='#combrbind'>combRbind</a></code>,
<code><a href='#drlapply'>drLapply</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- removeData -->   
<div class="tab-pane" id="removedata">

<h3 class="fref_title">Remove Key-Value Pairs from a Data Connection</h3>

<h4>Usage</h4>
<pre><code class="language-r">removeData(conn, keys)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>conn</dt>
  <dd>a kvConnection object</dd>
  <dt>keys</dt>
  <dd>a list of keys indicating which k/v pairs to
  remove</dd>
</dl>

  <h4>Description</h4>

  <p>Remove key-value pairs from a data connection</p>


  <h4>Note</h4>

  <p>This is generally not recommended for HDFS as it writes a
new file each time it is called, and can result in more
individual files than Hadoop likes to deal with.</p>




<h4>See also</h4>

<code><a href='#removedata'>removeData</a></code>, <code><a href='#localdiskconn'>localDiskConn</a></code>,
<code><a href='#hdfsconn'>hdfsConn</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- rhipeControl -->   
<div class="tab-pane" id="rhipecontrol">

<h3 class="fref_title">Specify Control Parameters for RHIPE Job</h3>

<h4>Usage</h4>
<pre><code class="language-r">rhipeControl(mapred = NULL, setup = NULL, combiner = FALSE, cleanup = NULL, orderby = "bytes", 
  shared = NULL, jarfiles = NULL, zips = NULL, jobname = "")</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>mapred,setup,combiner,cleanup,orderby,shared,jarfiles,zips,jobname</dt>
  <dd>arguments
  to <code>rhwatch</code> in RHIPE</dd>
</dl>

  <h4>Description</h4>

  <p>Specify control parameters for a RHIPE job.  See
<code>rhwatch</code> for details about each of the parameters.</p>



</div>


<!-- rrDiv -->   
<div class="tab-pane" id="rrdiv">

<h3 class="fref_title">Random Replicate Division</h3>

<h4>Usage</h4>
<pre><code class="language-r">rrDiv(nrows = NULL, seed = NULL)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>nrows</dt>
  <dd>number of rows each subset should have</dd>
  <dt>seed</dt>
  <dd>the random seed to use (experimental)</dd>
</dl>

  <h4>Value</h4>

  <p>a list to be used for the "by" argument to
<code><a href='#divide'>divide</a></code></p>


  <h4>Description</h4>

  <p>Specify random replicate division parameters for data
division</p>


  <h4>Details</h4>

  <p>The random replicate division method currently gets the
total number of rows of the input data and divides it by
<code>nrows</code> to get the number of subsets.  Then it
randomly assigns each row of the input data to one of the
subsets, resulting in subsets with approximately
<code>nrows</code> rows.  A future implementation will make each
subset have exactly <code>nrows</code> rows.</p>


  <h4>References</h4>

  <p><ul>
<li> <a href = 'http://www.datadr.org'>http://www.datadr.org</a> </li>
<li>
<a href = 'http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full'>Guha,
S., Hafen, R., Rounds, J., Xia, J., Li, J., Xi, B., &
Cleveland, W. S. (2012). Large complex data: divide and
recombine (D&R) with RHIPE. <em>Stat</em>, 1(1), 53-67.</a> </li>
</ul></p>

  <p></p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>,
<code><a href='#conddiv'>condDiv</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- sparkControl -->   
<div class="tab-pane" id="sparkcontrol">

<h3 class="fref_title">Specify Control Parameters for Spark Job</h3>

<h4>Usage</h4>
<pre><code class="language-r">sparkControl()</code></pre>

  <h4>Description</h4>

  <p>Specify control parameters for a Spark job.  See
<code>rhwatch</code> for details about each of the parameters.</p>



</div>


<!-- sparkDataConn -->   
<div class="tab-pane" id="sparkdataconn">

<h3 class="fref_title">Connect to Spark Data Source</h3>

<h4>Usage</h4>
<pre><code class="language-r">sparkDataConn(data = NULL, init = list(), verbose = TRUE)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>a data frame or list of key-value pairs</dd>
  <dt>init</dt>
  <dd>a named list of arguments to be passed to
  <code>sparkR.init</code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is
  being done</dd>
</dl>

  <h4>Description</h4>

  <p>Connect to Spark data source (experimental).</p>


  <h4>Note</h4>

  <p>This is currently a proof-of-concept.  It only allows
in-memory data to be initialized as a Spark RDD, which is
quite pointless for big data.  In the future, this will
allow connections to link to data on HDFS.</p>



</div>


<!-- updateAttributes -->   
<div class="tab-pane" id="updateattributes">

<h3 class="fref_title">Update Attributes of a 'ddo' or 'ddf' Object</h3>

<h4>Usage</h4>
<pre><code class="language-r">updateAttributes(obj, control = NULL)</code></pre>

<h4>Arguments</h4>
<dl>
  <dt>obj</dt>
  <dd>an object of class 'ddo' or 'ddf'</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend
  should handle things (most-likely parameters to
  <code>rhwatch</code> in RHIPE) - see
  <code><a href='#rhipecontrol'>rhipeControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>an object of class 'ddo' or 'ddf'</p>


  <h4>Description</h4>

  <p>Update attributes of a 'ddo' or 'ddf' object</p>


  <h4>Details</h4>

  <p>This function looks for missing attributes related to a ddo
or ddf (distributed data object or data frame) object and
runs MapReduce to update them.  These attributes include
"splitSizeDistn", "keys", "nDiv", "nRow", and
"splitRowDistn".  These attributes are useful for
subsequent computations that might rely on them.  The
result is the input modified to reflect the updated
attributes, and thus it should be used as <code>obj <-
updateAttributes(obj)</code>.</p>


  <h4>References</h4>

  <p>Bennett, Janine, et al. "Numerically stable, single-pass,
parallel statistics algorithms." Cluster Computing and
Workshops, 2009. <em>CLUSTER'09. IEEE International
Conference on.</em> IEEE, 2009.</p>




<h4>See also</h4>

<code><a href='#ddo'>ddo</a></code>, <code><a href='#ddf'>ddf</a></code>, <code><a href='#divide'>divide</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


   
   <ul class="pager">
      <li><a href="#" id="previous">&larr; Previous</a></li> 
      <li><a href="#" id="next">Next &rarr;</a></li> 
   </ul>
</div>


</div>
</div>

<hr>

<div class="footer">
   <p>&copy; Ryan Hafen, 2014</p>
</div>
</div> <!-- /container -->

<script src="assets/jquery/jquery.js"></script>
<script type='text/javascript' src='assets/custom/custom.js'></script>
<script src="assets/bootstrap/js/bootstrap.js"></script>
<script src="assets/custom/jquery.ba-hashchange.min.js"></script>
<script src="assets/custom/nav.js"></script>

</body>
</html>