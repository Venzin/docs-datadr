```

## Dealing with Data in D&R ##

### Key-Value Pairs ###

In D&R, data is partitioned into blocks.  Each block is represented as a *key-value pair*.  Collections of key-value pairs are *distributed data objects (ddo)*, or in the case of the value being a data frame, a *distributed data frame (ddf)*, and form the basic input and output types of all D&R operations.  This section introduces these concepts and illustrates how they are used in the `datadr` implementation.

The most basic data structure in D&R is a *key-value pair* which is simply a data structure with key and value elements, each of which can have any data structure.  

#### Key-value pairs in `datadr`

In `datadr`, key-value pairs are R lists with two elements, one for the key and one for the value.  For example,

```{r kv_pair_example}
# simple key-value pair example
list(1:5, rnorm(10))
```

is a key-value pair with integers 1-5 as the key and 10 random normals as the value.  Typically, a key is used as a unique identifier for the value.  For `datadr` it is recommended to make the key a simple string when possible.

#### Key-value pair collections

D&R data objects are made up of collections of key-value pairs.  In `datadr`, these are represented as lists of key-value pair lists.  As an example, consider the `iris` data set, which consists of measurements of 4 aspects for 50 flowers from each of 3 species of iris.  Suppose we would like to split the sepal measurements of the iris data into key-value pairs by species:

```{r by_species_kv}
# create by-species key-value pairs
irisKV <- list(
   list("setosa", subset(iris, Species == "setosa")[,c(1:2, 5)]),
   list("versicolor", subset(iris, Species == "versicolor")[,c(1:2, 5)]),
   list("virginica", subset(iris, Species == "virginica")[,c(1:2, 5)])
)
str(irisKV)
```

The result is a list of 3 key-value pairs.  We chose the species to be the key and the corresponding data frame of sepal measurements to be the value for each pair.

This example shows how we can partition our data into key-value pairs that have meaning -- each subset represents measurements for one species.  The ability to divide the data up into pieces allows us to distribute datasets that might be too large for a single disk across multiple machines, and also allows us to distribute computation, because in recombination we apply methods independently to each subset.

Here, we manually created the partition by species, but `datadr` provides simple mechanisms for specifying divisions, which we will cover [later in the tutorial](#division).  Prior to doing that, however, we need to discuss how collections of key-value pairs are represented in `datadr` as distributed data objects.

#### Applying functions to key-value pairs

As a final note on key-value pairs before moving on to [distributed data objects](#distributed-data-objects), this is a good place to discuss how functions are applied to key-value pairs in `datadr`.  There are many places in `datadr` methods where you can specify functions that you want to have applied to each key-value pair for various purposes.  These include:

- `transFn`: an argument to `ddo()`
- `preTransFn`, `postTransFn`, `bsvFn`, and `filterFn`: arguments to `divide()`
- `apply`: an argument to `recombine()`
- `panelFn` and `cogFn`: arguments to `makeDisplay()` in the `trelliscope` package

There is a general approach for flexibly specifying functions that operate on key-value pairs in `datadr`.  In some cases, you may want both the key and value to be available in the function for your code to operate on, but many times all you care about is applying the function to the value.  

To keep the writing of such functions simple, in `datadr`, a check is made to see how many formal arguments your function has.  If it has one argument, that argument is treated as the value.  If it has two arguments, the first is treated as the key and the second as the value.  Handling of this logic is done by a function `kvApply(fn, kvPair)`.

For example, suppose I want to apply a function computing the mean sepal length to the first key-value pair of the `irisKV` data:

```{r kvapply_value}
# kvApply example operating on just value
meanSepalLength1 <- function(v)
   mean(v$Sepal.Length)
   
kvApply(meanSepalLength1, irisKV[[1]])
```

Here my function `meanSepalLength1` only takes one argument so it is passed the value, which is the data frame of sepal measurements for this subset.  

However, suppose there is information in the key that I would like to include in my computation or output.  For example, suppose I want a data frame of the mean tagged by its associated species (the key):

```{r kvapply_key_value}
# kvApply example operating on key and value
meanSepalLength2 <- function(k, v)
   data.frame(species = k, mean = mean(v$Sepal.Length))
   
kvApply(meanSepalLength2, irisKV[[1]])
```

My function `meanSepalLength2` now takes two arguments, and therefore `kvApply` will provide it both the key and the value, available in the function as `k` and `v`, and the function can use the key to get the species to put in the resulting data frame.

<div class="alert alert-warning">
Keep in mind that this is how things are done for all supplied functions to arguments of <code>datadr</code> methods that operate on key-value pairs: functions with two arguments are passed the key and value, and functions with one argument are passed only the value.
</div>

<!-- The reason for this construct is that in our experience we want to apply most functions to just the value, but there always arise cases where we want information in the key as well.  We could simply have all functions expect one argument which is the key-value pair list, but then there is too much subsetting to get keys and values that is tedious and makes code less readable. -->

### Distributed Data Objects ###

In `datadr`, a collection of key-value pairs along with attributes about the collection constitute a distributed data object (ddo).  Most `datadr` operations require "ddo" objects, and hence it is important to represent key-value pair collections as such.

#### Initializing a "ddo" object

To initialize a collection of key-value pairs as a distributed data object, we use the `ddo()` function:

```{r create_ddo, message=FALSE}
# create ddo object from irisKV
irisDdo <- ddo(irisKV)
```

`ddo()` simply takes the collection of key-value pairs and attaches additional attributes to the resulting "ddo" object.  Note that in this example, since the data is in memory, we are supplying the data directly as the argument to `ddo()`.  For larger datasets stored in more scalable backends, instead of passing the data directly, a connection that points to where the key-value pairs are stored is provided.  This is discussed in more detail in the [Store/Compute Backends](#backend-choices) sections.

Objects of class "ddo" have several methods that can be invoked on them.  The most simple of these is a print method:

```{r print_ddo}
irisDdo
```

The print method shows several attributes that have been computed for the data.

#### "ddo" attributes

From the printout of `irisDdo`, we see that a "ddo" object has several attributes.  The most basic ones:

- `totSize`: The total size of the data is 6 KB (that's some big data!)
- `nDiv`: There are 3 divisions

We can look at the keys with

```{r ddo_keys}
# look at irisDdo keys
getKeys(irisDdo)
```

We can also get an example key-value pair with

```{r example_kv}
# look at an example key-value pair of irisDdo
kvExample(irisDdo)
```

`kvExample` is useful for obtaining a subset key-value pair against which we can test out different analytical methods before applying them across the entire data set.

The other attributes, `splitSizeDistn` and `bsvInfo` are empty.  `bsvInfo` provides information about between subset variables (BSVs), which we will discuss [later](#between-subset-variables).  

The `splitSizeDistn` attribute provides information about the quantiles of the distribution of the size of each division.  With very large data sets with a large number of subsets, this can be useful for getting a feel for how uniform the subset sizes are.  

The `splitSizeDistn` attribute and more that we will see in the future are not computed by default when `ddo()` is called.  This is because it requires a computation over the data set, which can take some time with very large datasets, and may not always be desired or necessary.

#### Updating attributes

If you decide at any point that you would like to update the attributes of your "ddo" object, you can call:

```{r update_attrs_iris_ddo, message=FALSE}
# update irisDdo attributes
irisDdo <- updateAttributes(irisDdo)
irisDdo
```

The `splitSizeDistn` attribute is now available.  We can look at it with the accessor `splitSizeDistn()`:

```{r plot_iris_split_size, fig.height=4, echo=2:3}
par(mar = c(4.1, 4.1, 1, 0.2))
# plot distribution of the size of the key-value pairs
plot(splitSizeDistn(irisDdo))
```

Another way to get updated attributes is at the time the "ddo" object is created, by setting `update = TRUE`:

```{r update_iris_ddo, message=FALSE}
# update at the time ddo() is called
irisDdo <- ddo(irisKV, update = TRUE)
```

#### Note about storage and computation

Notice the first and final lines of output from the `irisDdo` object printout.  It states that the object is of class "kvMemory" (key-value pairs in memory), and that it has an "in-memory data connection".  Here we are playing with a very small data set, but this package is designed to scale.  

We will talk about other backends for storing and processing larger data sets that don't fit in memory or even on your workstation's disk.  The key here is that the interface always stays the same, regardless of whether we are working with terabytes of kilobytes of data.

#### Accessing subsets

We can access subsets of the data by key or by index:

```{r subsetting_example}
str(irisDdo[["setosa"]])
str(irisDdo[[1]])
```

```{r multiple_subsetting_example}
str(irisDdo[c("setosa", "virginica")])
str(irisDdo[1:2])
```

Accessing by key is much simpler when the key is a character string, but subsetting works even when passing a list of non-string keys.

### Distributed Data Frames ###

Key-value pairs in distributed data objects can have any structure.  If we constrain the values to be data frames or readily transformable into data frames, we can represent the object as a distributed data frame (ddf).  Having a uniform data frame structure for the values provides several benefits and data frames are required for specifying division methods.

#### Initializing a "ddf" object

Our `irisKV` data we created earlier has values that are data frames, so we can cast it as a distributed data frame like this:

```{r iris_ddf, message=FALSE}
# create ddf object from irisKV
irisDdf <- ddf(irisKV, update = TRUE)
irisDdf
```

#### "ddf" attributes

The printout of `irisDdf` above shows data-frame-related attributes (which were automatically updated because we specified `update = TRUE`) in addition to the "ddo" attributes we saw earlier.  These include:

- `vars`: a list of the variables
- `transFn`: a transformation function (more on this later)
- `nrow`: the total number of rows in the data set
- `splitRowDistn`: which is similar to `splitSizeDistn`, except that it is the distribution of the number of rows of data in each subset
- `summary` attribute holds summary statistics about each variable in the data frame

The `summary` attribute can be useful for later computations, where doing it once up front is more efficient than .  A good example is quantile estimation (see `?drQuantile`), where the range is required to get a good quantile approximation.  Summary statistics are all computed simultaneously in one MapReduce job with a call to `updateAttributes()`.  

The numerical summary statistics are computed using a numerically stable algorithm (cite).  Summary statistics include:

For each numeric variable: 
- `nna`: number of missing values
- `stats`: list of mean, variance, skewness, kurtosis
- `range`: min, max

For each categorical variable:
- `nobs`: number of observations
- `nna`: number of missing values
- `freqTable`: a data frame containing a frequency table

Summaries can be accessed by:

```{r iris_summary}
# look at irisDdf summary stats
summary(irisDdf)
```

This prints the summary statistics.  For categorical variables, the top four values and their frequency is pritned.  To access the values themselves, we can do, for example:

```{r iris_summary2}
summary(irisDdf)$Sepal.Length
```

or:

```{r iris_summary3}
summary(irisDdf)$Species
```

#### Data frame-like "ddf" methods

Note that with an object of class "ddf", you can use some of the methods that apply to regular data frames:

```{r ddf_df_methods}
nrow(irisDdf)
ncol(irisDdf)
names(irisDdf)
```

#### Passing a data frame to `ddo()` and `ddf()`

It is worth noting that it is possible to pass a single data frame to `ddo()` or `ddf()`.  The result is a single key-value pair with the data frame as the value, and "" as the key.  This is an option strictly for convenience and with the idea that further down the line operations will be applied that split the data up into a more useful set of key-value pairs.  Here is an example:

```{r iris_df_ddf, message=FALSE}
# initialize ddf from a data frame
irisDf <- ddf(iris, update = TRUE)
```

This of course only makes sense for data small enough to fit in memory in the first place.  In the [backends](#small-memory--cpu) section, we will discuss other backends for larger data and how data can be added to objects in these cases.

#### Coercing to "ddf" with a Transformation Function

There may be times where we want to create a ddf but we have data with values that either aren't stored as a data frame or we don't want to store as a data frame, but we want them to act like data frames.  In this case, there is an argument to `ddf()` called `transFn` which transforms the data into a data frame prior to any computation is carried out on it.

Consider the following collection of key-value pairs, for example:

```{r unstruct}
# example of some "less-structured" key-value pairs
people <- list(
   list("fred", 
      list(age = 74, statesLived = c("NJ", "MA", "ND", "TX"))
   ),
   list("bob", 
      list(age = 42, statesLived = "NJ")
   )
)
```

The values are lists, not data frames.  In this case, the values are easily coercible to data frames using `as.data.frame`:

```{r asdataframe}
# cast first value as data frame
as.data.frame(people[[1]][[2]])
```

Since `people` is a list of key-value pairs, we extract the first pair with `[[1]]`, and then get the value of the pair with `[[2]]`.

We see that `as.data.frame` is able to coerce the list to a data frame.  So if we want to treat `people` as a distributed data frame, we might simply provide `as.data.frame` as the transformation function:

```{r ddf_transfn, message=FALSE}
# ddf with transFn
peopleDdf <- ddf(people, transFn = as.data.frame)
```

`ddf()` actually tries `as.data.frame` if the default (`identity`) does not yield a data frame:

```{r ddf_transfn_default, message=FALSE}
# ddf tries as.data.frame for transFn by default
peopleDdf <- ddf(people)
```

With this data representation, is what things will look like to `datadr` methods that need the data to be a data frame:

```{r ddf_example_transform}
# get a ddf key-value pair with transFn applied
kvExample(peopleDdf, transform = TRUE)
```

<!-- The `transform = TRUE` argument tells `kvExample()` to apply `transFn` to the result. -->

But how the data actually is stored is still the same:

```{r ddf_example_untransform}
# data is still stored unstructured (pre transFn)
kvExample(peopleDdf)
```

More complex examples will certainly arise where a non-trivial transformation function is required to cast the data as a data frame.  

This transformation is honored in `datadr` methods such as `divide()` and `updateAttributes()`.

### Common Data Operations ###

The majority of this documentation will cover division and recombination, but here, we present some methods that are available for common data operations that come in handy for manipulating data in various ways.

#### lapply

It is convenient to be able use the familiar `lapply()` approach to apply a function to each key-value pair.  An `lapply()` method, called `drLapply()` is available for "ddo" and "ddf" objects.  The function you specify follows the same convention as described earlier (if it has one argument, it is applied to the value only, if it has two arguments, it is applied to the key and value).  A "ddo" object is returned.

Here is an example of using `drLapply()` to the `irisDdf` data:

```{r lapply, message=FALSE}
# get the mean Sepal.Width for each key-value pair in irisDdf
means <- drLapply(irisDdf, function(x) mean(x$Sepal.Width))
# turn the resulting "ddo" object into a list
as.list(means)
```

#### Filter

A `drFilter()` function is available which takes a function that is applied to each key-value pair (using the `kvApply()` convention).  If the function returns `TRUE`, that key-value pair will be included in the resulting "ddo" or "ddf" object, if `FALSE`, it will not.

Here is an example that keeps all subsets with mean sepal width less than 3:

```{r filter, message=FALSE}
# keep subsets with mean sepal width less than 3
drFilter(irisDdf, function(v) mean(v$Sepal.Width) < 3)
```

#### Join

The `drJoin()` operation takes multiple input "ddo" or "ddf" objects and merges their values by key.  This is a very useful function when there are multiple input sources that you would like to group together.

Suppose with the iris data that we have two separate input sources, one that reports the sepal width and another that reports the sepal length for each species:

```{r join, message=FALSE}
# create two new "ddo" objects that contain sepal width and sepal length
sw <- drLapply(irisDdf, function(x) x$Sepal.Width)
sl <- drLapply(irisDdf, function(x) x$Sepal.Length)
```

An example subset of `sw` looks like this:

```{r join2}
sw[[1]]
```

Both `sw` and `sl` have the same set of keys, and the value is a vector of either the sepal width or length.  To join them together, we can call `drJoin()`.  This function takes any number of "ddo" or "ddf" input objects, and they must be named.  It also optionally takes a `postTransFn` argument, which allows a transformation function to be applied the joined result.

By default, `drJoin()` groups the various data sources by key, and the resulting value is a named list, where each element of the list is the value from each data source.  For example, to join the `sw` and `sl` data, we get the following:

```{r join3, message=FALSE}
# join sw and sl by key
joinRes <- drJoin(Sepal.Width=sw, Sepal.Length=sl)
# look at first key-value pair
joinRes[[1]]
```

The resulting object, `joinRes`, has subsets with the same keys, but the values are now named lists that consist of the values from both data sets.

Here is an example of specifying `postTransFn` to turn the resulting values of the join operation into a data frame:

```{r join4, message=FALSE}
# join sw and sl by key and turn the result into a data frame
joinRes <- drJoin(Sepal.Width=sw, Sepal.Length=sl, postTransFn = as.data.frame)
# look at first key-value pair
joinRes[[1]]
```

#### Sample

It can be useful to create a new data set of randomly sampled subsets of a large data set.  The `drSample()` function provides for this.  Currently, it is as simple as specifying the fraction of subsets you would like the resulting data set to have:

```{r sample, message=FALSE}
set.seed(1234)
drSample(irisDdf, fraction = 0.25)
```

In the future, we will add the capability to sample the data with respect to [between-subset-variables](#between-subset-variables).

